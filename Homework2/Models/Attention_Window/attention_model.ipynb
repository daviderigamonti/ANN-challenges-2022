{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"#!pip install -U dill==0.3.5.1\n#!pip install -U tensorflow-io==0.27.0\n#!pip install -U tensorflow-transform==1.11.0\n#!pip install -U tensorflow==2.10.0\n!pip install keras-self-attention\nimport tensorflow as tf\nprint(tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\nimport pandas as pd\nimport seaborn as sns\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split, ShuffleSplit, StratifiedShuffleSplit\nfrom sklearn.utils import indexable, _safe_indexing, shuffle\nfrom sklearn.utils.validation import _num_samples\nfrom math import ceil, floor\nfrom itertools import chain\nimport warnings\nfrom keras_self_attention import SeqSelfAttention, SeqWeightedAttention ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\ntf.get_logger().setLevel('ERROR')\n\nplt.rc('font', size=16)\n\ntfk = tf.keras\ntfkl = tf.keras.layers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defines","metadata":{}},{"cell_type":"markdown","source":"### Seed","metadata":{}},{"cell_type":"code","source":"# Random seed for reproducibility\nseed = 1337\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Constants","metadata":{}},{"cell_type":"markdown","source":"#### Imports","metadata":{}},{"cell_type":"code","source":"dataset_location = '/kaggle/input/training-dataset-homework2/'\nx_name = 'x_train.npy'\ny_name = 'y_train.npy'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"classes = 12\nfeatures = 6\ninput_shape = (None, 6)\n\nbatch_size = 64\nepochs = 1000\n\nlabel_mapping = {\n    \"Wish\" : 0,\n    \"Another\" : 1,\n    \"Comfortably\" : 2,\n    \"Money\" : 3,\n    \"Breathe\" : 4,\n    \"Time\" : 5,\n    \"Brain\" : 6,\n    \"Echoes\" : 7,\n    \"Wearing\" : 8,\n    \"Sorrow\" : 9,\n    \"Hey\" : 10,\n    \"Shine\" : 11\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utility Functions","metadata":{}},{"cell_type":"markdown","source":"#### Custom scipy","metadata":{}},{"cell_type":"code","source":"def _validate_shuffle_split(n_samples, test_size, train_size, default_test_size=None):\n    \"\"\"\n    Validation helper to check if the test/test sizes are meaningful wrt to the\n    size of the data (n_samples)\n    \"\"\"\n    if test_size is None and train_size is None:\n        test_size = default_test_size\n\n    test_size_type = np.asarray(test_size).dtype.kind\n    train_size_type = np.asarray(train_size).dtype.kind\n\n    if (\n        test_size_type == \"i\"\n        and (test_size >= n_samples or test_size <= 0)\n        or test_size_type == \"f\"\n        and (test_size <= 0 or test_size >= 1)\n    ):\n        raise ValueError(\n            \"test_size={0} should be either positive and smaller\"\n            \" than the number of samples {1} or a float in the \"\n            \"(0, 1) range\".format(test_size, n_samples)\n        )\n\n    if (\n        train_size_type == \"i\"\n        and (train_size >= n_samples or train_size <= 0)\n        or train_size_type == \"f\"\n        and (train_size <= 0 or train_size >= 1)\n    ):\n        raise ValueError(\n            \"train_size={0} should be either positive and smaller\"\n            \" than the number of samples {1} or a float in the \"\n            \"(0, 1) range\".format(train_size, n_samples)\n        )\n\n    if train_size is not None and train_size_type not in (\"i\", \"f\"):\n        raise ValueError(\"Invalid value for train_size: {}\".format(train_size))\n    if test_size is not None and test_size_type not in (\"i\", \"f\"):\n        raise ValueError(\"Invalid value for test_size: {}\".format(test_size))\n\n    if train_size_type == \"f\" and test_size_type == \"f\" and train_size + test_size > 1:\n        raise ValueError(\n            \"The sum of test_size and train_size = {}, should be in the (0, 1)\"\n            \" range. Reduce test_size and/or train_size.\".format(train_size + test_size)\n        )\n\n    if test_size_type == \"f\":\n        n_test = ceil(test_size * n_samples)\n    elif test_size_type == \"i\":\n        n_test = float(test_size)\n\n    if train_size_type == \"f\":\n        n_train = floor(train_size * n_samples)\n    elif train_size_type == \"i\":\n        n_train = float(train_size)\n\n    if train_size is None:\n        n_train = n_samples - n_test\n    elif test_size is None:\n        n_test = n_samples - n_train\n\n    if n_train + n_test > n_samples:\n        raise ValueError(\n            \"The sum of train_size and test_size = %d, \"\n            \"should be smaller than the number of \"\n            \"samples %d. Reduce test_size and/or \"\n            \"train_size.\" % (n_train + n_test, n_samples)\n        )\n\n    n_train, n_test = int(n_train), int(n_test)\n\n    if n_train == 0:\n        raise ValueError(\n            \"With n_samples={}, test_size={} and train_size={}, the \"\n            \"resulting train set will be empty. Adjust any of the \"\n            \"aforementioned parameters.\".format(n_samples, test_size, train_size)\n        )\n\n    return n_train, n_test\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_train_test_split(\n    *arrays,\n    test_size=None,\n    train_size=None,\n    random_state=None,\n    shuffle=True,\n    stratify=None,\n):\n    n_arrays = len(arrays)\n    if n_arrays == 0:\n        raise ValueError(\"At least one array required as input\")\n\n    arrays = indexable(*arrays)\n\n    n_samples = _num_samples(arrays[0])\n    n_train, n_test = _validate_shuffle_split(\n        n_samples, test_size, train_size, default_test_size=0.25\n    )\n\n    if shuffle is False:\n        if stratify is not None:\n            raise ValueError(\n                \"Stratified train/test split is not implemented for shuffle=False\"\n            )\n\n        train = np.arange(n_train)\n        test = np.arange(n_train, n_train + n_test)\n\n    else:\n        if stratify is not None:\n            CVClass = StratifiedShuffleSplit\n        else:\n            CVClass = ShuffleSplit\n\n        cv = CVClass(test_size=n_test, train_size=n_train, random_state=random_state)\n\n        train, test = next(cv.split(X=arrays[0], y=stratify))\n\n    return list(\n        chain.from_iterable(\n            (train, test) for a in arrays\n        )\n    )\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Windowing","metadata":{}},{"cell_type":"code","source":"def unroll_splice(x, y):\n    unique_labels, labels_count = np.unique(y, return_counts=True)\n    x_splices = [[] for _ in range(len(unique_labels))]\n    y_splices = [label for label in unique_labels]\n    \n    for x_samples, label in zip(x, y):\n        x_splices[label].extend(x_samples.tolist())\n        \n    return x_splices, y_splices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_sequences(\n    x, y, \n    window, # window: new window size\n    stride, # stride: interval between windows\n    padding = True, # padding: if False, throw an error if padding is required\n    preserve_window_size = False # preserve_window_size: if True, only generate new samples inside the old window\n):\n    \n    old_window = np.shape(x)[1]\n    overlap = window - stride\n    jump_length = window - overlap\n    \n    assert overlap >= 0\n    \n    dataset = []\n    labels = []\n    \n    if preserve_window_size:\n        assert window <= old_window\n        x_iter, y_iter = x, y\n    else:\n        x_iter, y_iter = unroll_splice(x, y)\n        \n    for samples, label in zip(x_iter, y_iter):\n        \n        new_samples = samples.copy()\n        \n        # Padding (not guaranteed to work)\n        if len(samples) % jump_length:\n            if padding:\n                padding_len = overlap\n                padding = [[0 for _ in range(np.shape(samples)[1])] for _ in range(padding_len)]\n                padding.extend(samples)\n                new_samples = padding\n            else:\n                print(\"NO PADDING >:(\")\n                assert False\n                \n        n_new_samples = len(new_samples) - window + 1\n        \n        for i in np.arange(0, n_new_samples, stride):\n            dataset.append(new_samples[i:i + window][:])\n            labels.append(label)\n    \n    return dataset, labels","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature tinkering","metadata":{}},{"cell_type":"code","source":"def drop_features(x, features):\n    dataset = []\n    for window in x:\n        window = window.T\n        window = [ value for i, value in enumerate(window) if i not in features ]\n        dataset.append(np.array(window).T)\n    return np.array(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_amplitude_feature_normalize(\n    x,\n    window_amp,\n    features = [i for i in range(features)], \n):\n    augmented_dataset = []\n    \n    for window in x:\n        window_size = np.shape(window)[0]\n        window = window.T\n        new_window = []\n        new_new_window = []\n\n        for i, f_samples in enumerate(window):\n            if i in features:\n\n                f_newsamples = []\n\n                for i in range(window_size):\n                    i_ = i - window_amp if i - window_amp > 0 else 0\n                    j_ = i + window_amp if i + window_amp <= window_size else window_size\n                    f_newsamples.append(np.mean(f_samples[i_:j_]))\n\n                f_samples /= np.max(np.abs(f_samples))\n                new_new_window.append(f_newsamples)\n            new_window.append(f_samples)\n        window = np.concatenate((new_window, new_new_window))\n        window = window.T\n        augmented_dataset.append(window)\n\n    return np.array(augmented_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_frequence_feature(\n    x,\n    features = [i for i in range(features)], \n):\n    augmented_dataset = []\n    \n    for window in x:\n        window_size = np.shape(window)[0]\n        window = window.T\n        new_window = []\n        new_new_window = []\n\n        for i, f_samples in enumerate(window):\n            if i in features:\n\n                f_newsamples = []\n                old = 0\n                \n                for sample in f_samples:\n                    f_newsamples.append(sample-old)\n                    old = sample\n                    \n                new_new_window.append(f_newsamples)\n            new_window.append(f_samples)\n        window = np.concatenate((new_window, new_new_window))\n        window = window.T\n        augmented_dataset.append(window)\n\n    return np.array(augmented_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### DAUG","metadata":{}},{"cell_type":"code","source":"def daug_reverse(\n    x, y,\n    size = 0.5, # Size of the inverted section from 0 to 1\n    frequence = 0.5, # Frequence of DAUG on dataset\n    only_augmented = False, # Return only augmented entries\n    features = [i for i in range(features)], classes = [i for i in range(classes)], # Masks for excluding features/classes\n    seed = 0\n):\n    rng = random.Random(seed)\n    augmented_dataset = []\n    labels = []\n    \n    for window, label in zip(x, y):\n        aug = False\n        if label in classes and rng.uniform(0, 1) <= frequence:\n            \n            aug = True\n            window_size = np.shape(window)[0]\n            window = window.T\n            new_window = []\n            \n            # Calculate reversing interval\n            reverse_size = int(np.rint(size * window_size))\n            reverse_size = reverse_size if reverse_size > 0 else 2\n            reverse_start = rng.randint(0, window_size - reverse_size)\n            reverse_end = reverse_start + reverse_size\n            \n            for i, f_samples in enumerate(window):\n                if i in features:\n                    f_samples = f_samples.tolist()\n                    \n                    # Reverse\n                    f_samples[reverse_start:reverse_end] = f_samples[reverse_start:reverse_end][::-1]\n                    \n                new_window.append(f_samples)\n            window = np.array(new_window).T\n            \n        if not only_augmented or aug:\n            augmented_dataset.append(window)\n            labels.append(label)\n            \n    return np.array(augmented_dataset), np.array(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def daug_noise(\n    x, y, \n    mean = 0, variance = 1, # Noise parameters\n    frequence = 0.5, # Frequence of DAUG on dataset\n    only_augmented = False, # Return only augmented entries\n    features = [i for i in range(features)], classes = [i for i in range(classes)], # Masks for excluding features/classes\n    seed = 0\n):\n    rng = random.Random(seed)\n    augmented_dataset = []\n    labels = []\n    \n    for window, label in zip(x, y):\n        aug = False\n        if label in classes and rng.uniform(0, 1) <= frequence:\n            \n            aug = True\n            window_size = np.shape(window)[0]\n            window = window.T\n            new_window = []\n            \n            for i, f_samples in enumerate(window):\n                if i in features:\n                    \n                    # Adding noise\n                    f_samples + np.random.normal(mean, variance, window_size)\n                    \n                new_window.append(f_samples)\n            window = np.array(new_window).T\n            \n        if not only_augmented or aug:\n            augmented_dataset.append(window)\n            labels.append(label)\n            \n    return np.array(augmented_dataset), np.array(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cutmix_element(x, y, label, rng):\n    pool = [w for w, l in zip(x,y) if l == label]\n    idx = rng.randint(0, len(pool) - 1)\n    return pool[idx].T\n\ndef daug_cutmix(\n    x, y, \n    min_max_size = (0.2, 0.4), # Minimum and maximum size for cutmix source, this is ignore if full_embed = True\n    full_embed = False, # Embeds all of the cutmix source entry into the receiving (WARNING! It changes the size of the entry, not recommended with custom feature masks)\n    change_for_each_feature = False, # If true, eachs feature gets its cutmix from a different source/size/position\n    frequence = (0.5, 0.5), # Frequence of DAUG on dataset and frequence of DAUG on features\n    only_augmented = False, # Return only augmented entries\n    features = [i for i in range(features)], classes = [i for i in range(classes)], # Masks for excluding features/classes\n    seed = 0\n):\n    rng = random.Random(seed)\n    augmented_dataset = []\n    labels = []\n    \n    for window, label in zip(x, y):\n        aug = False\n        if label in classes and rng.uniform(0, 1) <= frequence[0]:\n            aug = True\n            window_size = np.shape(window)[0]\n            window = window.T\n            new_window = []\n            \n            # Get new cutmix source element\n            cutmix_el = get_cutmix_element(x, y, label, rng)\n            \n            # Calculate cutmix source size + position and cutmix destination position\n            if full_embed:\n                cutmix_position = rng.randint(0, window_size)\n            else:\n                cutmix_size = int(np.rint(rng.uniform(*min_max_size) * window_size))\n                cutmix_size = cutmix_size if cutmix_size > 0 else 1\n                cutmix_position_s = rng.randint(0, window_size - cutmix_size)\n                cutmix_position_r = rng.randint(0, window_size - cutmix_size)\n                \n            \n            for i, f_samples in enumerate(window):\n                if i in features and rng.uniform(0, 1) <= frequence[1]:\n                    \n                    # Apply cutmix to feature\n                    if full_embed:\n                        np.insert(f_samples, cutmix_position, cutmix_el[i])\n                    else:\n                        f_samples[cutmix_position_r:cutmix_position_r + cutmix_size] = cutmix_el[i][cutmix_position_s:cutmix_position_s + cutmix_size]\n                        \n                    # Change cutmix parameters if the source is different for each feature\n                    if change_for_each_feature:\n                        cutmix_size = int(np.rint(rng.uniform(*min_max_size) * window_size))\n                        cutmix_size = cutmix_size if cutmix_size > 0 else 1\n                        cutmix_el = get_cutmix_element(x, y, label, rng)\n                        cutmix_position_s = rng.randint(0, window_size - cutmix_size)\n                        cutmix_position_r = rng.randint(0, window_size - cutmix_size)\n                        \n                new_window.append(f_samples)\n            window = np.array(new_window).T\n            \n        if not only_augmented or aug:\n            augmented_dataset.append(window)\n            labels.append(label)\n            \n    return np.array(augmented_dataset), np.array(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualization","metadata":{}},{"cell_type":"code","source":"def inspect_multivariate(x, y, idx=None):\n    if(idx == None):\n        idx = np.random.randint(0, len(x))\n    print(f\"{y[idx]} --> {np.argmax(y[idx])}\")\n    df = pd.DataFrame(x[idx,:,:])\n    df.plot(subplots=True, figsize=(17, 9))\n    plt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inspect_timewise(total, bars=True, features=[i for i in range(6)]):\n    vis = np.transpose(np.concatenate(total))\n    print(vis.shape)\n    plt.rcParams[\"figure.figsize\"] = [15, 30]\n    plt.rcParams[\"figure.autolayout\"] = True\n    fig, axs = plt.subplots(len(features))\n    ax_idxs = {i:n for n, i in enumerate(features)}\n    for idx in features:\n        ax_idx = ax_idxs[idx]\n        df = pd.DataFrame(vis[idx,:])\n        axs[ax_idx].plot(df)\n        if bars:\n            l=0\n            for i in label_counts:\n                l = l + i*36\n                axs[ax_idx].axvline(l, color='red')\n            axs[ax_idx].text(s = f\"Feature {idx+1}\", x = 80000, y = 30000)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_classes_performance(model, x_val, y_val):\n    data_list = []\n    label_list = []\n    batch_index = 0\n\n    data_array = np.array(x_val)\n    label_array = np.array(y_val)   \n    label_values = np.argmax(label_array, axis=1)\n    predictions = model.predict(data_array)\n    predictions_label = np.argmax(predictions, axis=1)\n    \n    report = classification_report(label_values, predictions_label)\n    print(report)\n    \n    # Compute the confusion matrix\n    cm = confusion_matrix(np.argmax(y_val, axis=-1), np.argmax(predictions, axis=-1))\n\n    # Compute the classification metrics\n    accuracy = accuracy_score(np.argmax(y_val, axis=-1), np.argmax(predictions, axis=-1))\n    precision = precision_score(np.argmax(y_val, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n    recall = recall_score(np.argmax(y_val, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n    f1 = f1_score(np.argmax(y_val, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n    print('Accuracy:', accuracy.round(4))\n    print('Precision:', precision.round(4))\n    print('Recall:', recall.round(4))\n    print('F1:', f1.round(4))\n\n    # Plot the confusion matrix\n    plt.figure(figsize=(10,8))\n    sns.heatmap(cm.T, cmap='Blues', xticklabels=list(label_mapping.keys()), yticklabels=list(label_mapping.keys()))\n    plt.xlabel('True labels')\n    plt.ylabel('Predicted labels')\n    plt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history(history):\n    best_epoch = np.argmax(history['val_accuracy'])\n    plt.figure(figsize=(17,4))\n    plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n    plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n    plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n    plt.title('Categorical Crossentropy')\n    plt.legend()\n    plt.grid(alpha=.3)\n    plt.show()\n\n    plt.figure(figsize=(17,4))\n    plt.plot(history['accuracy'], label='Training accuracy', alpha=.8, color='#ff7f0e')\n    plt.plot(history['val_accuracy'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\n    plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n    plt.title('Accuracy')\n    plt.legend()\n    plt.grid(alpha=.3)\n    plt.show()\n\n    plt.figure(figsize=(17,4))\n    plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n    plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n    plt.legend()\n    plt.grid(alpha=.3)\n    plt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"x_total = np.load(dataset_location + x_name)\ny_total = np.load(dataset_location + y_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"code","source":"# Weight classes\nclass_weights = {}\n_, label_counts = np.unique(y_total, return_counts=True)\nclass_mean = sum(label_counts) / len(label_counts)\nfor i, n in enumerate(label_counts):\n    class_weights[i] = (1 / n) * class_mean\nprint(class_weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_masks = my_train_test_split(\n    x_total,\n    y_total,\n    test_size=0.2,\n    stratify=y_total,\n    random_state=seed\n)\n\nfor mask in split_masks:\n    mask.sort()\n\nx_val = x_total[split_masks[1]]\nx_train = np.delete(x_total, split_masks[1], axis=0)\n\ny_val = y_total[split_masks[3]]\ny_train = np.delete(y_total, split_masks[3], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sanity checks\nassert (split_masks[0]==split_masks[2]).all() and (split_masks[1]==split_masks[3]).all()\nassert np.intersect1d(split_masks[0], split_masks[1]).size == 0\nassert len(split_masks[0]) + len(split_masks[1]) == len(x_total)\nassert np.array_equal(x_train, np.squeeze(x_total[split_masks[0]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, y_train = build_sequences(x_train, y_train, window=36, stride=1, padding=False, preserve_window_size=False)\nx_train = np.array(x_train)\ny_train = np.array(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nshuffle(x_train, y_train, random_state=seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DAUG pipeline","metadata":{}},{"cell_type":"code","source":"#x_train = drop_features(x_train, [0])\n#x_val = drop_features(x_val, [0])\n\n#x_train = generate_frequence_feature(x_train, [0, 1, 2, 3, 4, 5])\n#x_val = generate_frequence_feature(x_val, [0, 1, 2, 3, 4, 5])\n\n#x_train = generate_frequence_feature(x_train, [6, 7, 8, 9, 10, 11])\n#x_val = generate_frequence_feature(x_val, [6, 7, 8, 9, 10, 11])\n\n#x_train = generate_amplitude_feature_normalize(x_train, 12, [0, 1, 2, 3, 4, 5])\n#x_val = generate_amplitude_feature_normalize(x_val, 12, [0, 1, 2, 3, 4, 5])\n\n# Drop original features\n#x_train = drop_features(x_train, [0, 1, 2, 3, 4, 5])\n#x_val = drop_features(x_val, [0, 1, 2, 3, 4, 5])\n\n# Adapt to number of features\n#input_shape = (None, 18)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x_train, y_train = daug_reverse(x_train, y_train, 1, 0.3, False, classes = [1, 4, 5], seed=seed)\n#x_train_aug3, y_train_aug3 = daug_reverse(x_train, y_train, 1, 0.5, True, classes = [1, 4, 5], features = [i for i in range(6)], seed=seed)\n#np.append(x_train, x_train_aug1)\n#np.append(y_train, y_train_aug1)\n#x_train, y_train = daug_noise(x_train, y_train, 10, 1, 0.2, False, seed=seed)\n#x_train_aug2, y_train_aug2 = daug_noise(x_train_aug1, y_train_aug1, 0, 0.75, 1, True, features = [i for i in range(6)], seed=seed)\n#np.append(x_train, x_train_aug2)\n#np.append(y_train, y_train_aug2)\n#x_train, y_train = daug_cutmix(x_train, y_train, (0.2, 0.5), False, False, (0.1, 1), False, seed=seed)\n#x_train_aug3, y_train_aug3 = daug_cutmix(x_train, y_train, (0.2, 0.5), False, False, (0.3, 1), True, classes = [1, 4, 5, 7, 11], features = [i for i in range(6)], seed=seed)\n#x_train = np.append(x_train, x_train_aug3, axis=0)\n#y_train = np.append(y_train, y_train_aug3, axis=0)\n\nshuffle(x_train, y_train, random_state=seed)\nprint(\":D Stop shuffle from printing 1km of stuff\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### One hot","metadata":{}},{"cell_type":"code","source":"y_train = tfk.utils.to_categorical(y_train)\ny_val = tfk.utils.to_categorical(y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization","metadata":{}},{"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)\n\nprint(x_val.shape)\nprint(y_val.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inspect_timewise(x_total)\ninspect_multivariate(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"def build_LSTM_classifier(input_shape, classes):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n    \n    # Preprocessing\n    pre = input_layer\n    pre = tf.keras.layers.Normalization(axis=2, name=\"Norm\")(pre)\n    \n    # Feature extractor\n    lstm = pre\n    lstm = tfkl.LSTM(128, return_sequences=True)(lstm)\n    lstm = tfkl.LSTM(128)(lstm)\n    lstm = tfkl.Dropout(.5, seed=seed)(lstm)\n    \n    # Classifier\n    classifier = lstm\n    classifier = tfkl.Dense(128, activation='relu')(classifier)\n    classifier = tfkl.Dense(classes, activation='softmax')(classifier)\n\n    # Connect input and output through the Model class\n    output_layer = classifier\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    # Compile the model\n    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n\n    # Return the model\n    return model","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_BiLSTM_classifier(input_shape, classes):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n    \n    # Preprocessing\n    pre = input_layer\n    pre = tf.keras.layers.Normalization(axis=2, name=\"Norm\")(pre)\n\n    # Feature extractor\n    bilstm = pre\n    bilstm = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True))(bilstm)\n    bilstm = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True))(bilstm)\n    bilstm = tfkl.Bidirectional(tfkl.LSTM(256))(bilstm)\n    bilstm = tfkl.Dropout(.5, seed=seed)(bilstm)\n\n    # Classifier\n    classifier = bilstm\n    classifier = tfkl.Dense(128, activation='relu')(classifier)\n    classifier = tfkl.Dense(classes, activation='softmax')(classifier)\n\n    # Connect input and output through the Model class\n    output_layer = classifier\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    # Compile the model\n    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n\n    # Return the model\n    return model","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_old1DCNN_classifier(input_shape, classes):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n    \n    pre = input_layer\n    pre = tf.keras.layers.Normalization(axis=2, name=\"Norm\")(pre)\n\n    # Feature extractor\n    cnn = pre\n    cnn = tfkl.Conv1D(64, 3, padding='same', activation='relu')(cnn)\n    cnn = tfkl.MaxPooling1D()(cnn)\n    cnn = tfkl.Dropout(.2, seed=seed)(cnn)\n    cnn = tfkl.Conv1D(64, 3, padding='same', activation='relu')(cnn)\n    cnn = tfkl.MaxPooling1D()(cnn)\n    cnn = tfkl.Dropout(.2, seed=seed)(cnn)\n    cnn = tfkl.Conv1D(64, 3, padding='same', activation='relu')(cnn)\n    cnn = tfkl.MaxPooling1D()(cnn)\n    cnn = tfkl.Dropout(.2, seed=seed)(cnn)\n    cnn = tfkl.Conv1D(64, 3, padding='same', activation='relu')(cnn)\n    cnn = tfkl.MaxPooling1D()(cnn)\n    cnn = tfkl.Dropout(.2, seed=seed)(cnn)\n    cnn = tfkl.Conv1D(64, 3, padding='same', activation='relu')(cnn)\n    cnn = tfkl.GlobalAveragePooling1D()(cnn)\n    cnn = tfkl.Dropout(.2, seed=seed)(cnn)\n\n    # Classifier\n    classifier = cnn\n    classifier = tfkl.Dense(\n        128, activation='relu',\n        kernel_regularizer=tfk.regularizers.L1(0.01),\n        activity_regularizer=tfk.regularizers.L2(0.01)\n    )(classifier)\n    classifier = tfkl.Dense(classes, activation='softmax')(classifier)\n\n    # Connect input and output through the Model class\n    output_layer = classifier\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    # Compile the model\n    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n\n    # Return the model\n    return model","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_1DCNN_classifier(input_shape, classes):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n    \n    pre = input_layer\n    pre = tf.keras.layers.Normalization(axis=2, name=\"Norm\")(pre)\n\n    # Feature extractor\n    cnn = pre\n    cnn = tfkl.Conv1D(128, 3, padding='same', activation='relu')(cnn)\n    cnn = tfkl.MaxPooling1D()(cnn)\n    cnn = tfkl.Dropout(.1, seed=seed)(cnn)\n    cnn = tfkl.Conv1D(128, 3, padding='same', activation='relu')(cnn)\n    cnn = tfkl.MaxPooling1D()(cnn)\n    cnn = tfkl.Dropout(.1, seed=seed)(cnn)\n    cnn = tfkl.Conv1D(128, 3, padding='same', activation='relu')(cnn)\n    cnn = tfkl.MaxPooling1D()(cnn)\n    #cnn = tfkl.GlobalAveragePooling1D()(cnn)\n    cnn = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True))(cnn)\n    cnn = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True))(cnn)\n    cnn = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True))(cnn)\n    cnn = tfkl.Bidirectional(tfkl.LSTM(128))(cnn)\n    \n    \n    cnn = tfkl.Dropout(.4, seed=seed)(cnn)\n\n    # Classifier\n    classifier = cnn\n    classifier = tfkl.Dense(\n        128, activation='relu',\n        kernel_regularizer=tfk.regularizers.L1(0.01),\n        activity_regularizer=tfk.regularizers.L2(0.01)\n    )(classifier)\n    classifier = tfkl.Dense(classes, activation='softmax')(classifier)\n\n    # Connect input and output through the Model class\n    output_layer = classifier\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    # Compile the model\n    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n\n    # Return the model\n    return model","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_attention1_classifier(input_shape, classes):\n    \n\n    model = tfk.models.Sequential()\n    \n    # Build the neural network layer by layer\n    model.add(tfkl.Input(shape=input_shape, name='Input'))\n    \n    model.add(tf.keras.layers.Normalization(axis=2, name=\"Norm\"))\n    \n    model.add(tfkl.Conv1D(128, 3, padding='same', activation='relu'))\n    model.add(tfkl.MaxPooling1D())\n    model.add(tfkl.Dropout(.2, seed=seed))\n    model.add(tfkl.Conv1D(128, 3, padding='same', activation='relu'))\n    model.add(tfkl.MaxPooling1D())\n    model.add(tfkl.Dropout(.2, seed=seed))\n\n    # Feature extractor\n    model.add(tfkl.Bidirectional(tfkl.LSTM(256, return_sequences=True)))\n    model.add(SeqSelfAttention(\n        #attention_width=12,\n        #attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n        #attention_activation='softmax',\n    ))\n    model.add(tfkl.Dropout(.2, seed=seed))\n    model.add(tfkl.Bidirectional(tfkl.LSTM(256, return_sequences=True)))\n    model.add(SeqSelfAttention(\n        #attention_width=24,\n        #attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n        #attention_activation='softmax',\n    ))\n    model.add(tfkl.Dropout(.2, seed=seed))\n    model.add(tfkl.Bidirectional(tfkl.LSTM(256, return_sequences=True)))\n    model.add(SeqSelfAttention(\n        #attention_width=36,\n        #attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n        #attention_activation='softmax',\n    ))\n    \n    model.add(tfkl.GlobalAveragePooling1D())\n    \n    model.add(tfkl.Dropout(.5, seed=seed))\n\n    # Classifier\n    model.add(tfkl.Dense(\n        128, activation='relu',\n        kernel_regularizer=tfk.regularizers.L1(0.01),\n        activity_regularizer=tfk.regularizers.L2(0.01)\n    ))\n    model.add(tfkl.Dense(classes, activation='softmax'))\n\n    # Connect input and output through the Model class\n    #model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    # Compile the model\n    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n\n    # Return the model\n    return model","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_attention2_classifier(input_shape, classes):\n    \n\n    model = tfk.models.Sequential()\n    \n    # Build the neural network layer by layer\n    model.add(tfkl.Input(shape=input_shape, name='Input'))\n    \n    model.add(tf.keras.layers.Normalization(axis=2, name=\"Norm\"))\n    \n    model.add(tfkl.Conv1D(128, 3, padding='same', activation='relu'))\n    model.add(tfkl.MaxPooling1D())\n    model.add(tfkl.Dropout(.3, seed=seed))\n\n    # Feature extractor\n    model.add(tfkl.Bidirectional(tfkl.LSTM(256, return_sequences=True), name = \"LSTM1\"))\n    model.add(SeqSelfAttention(\n        #attention_width=12,\n        #attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n        #attention_activation='softmax',\n        name = \"LSTM1-a\"\n    ))\n    model.add(tfkl.Dropout(.3, seed=seed, name = \"LSTM1-d\"))\n    model.add(tfkl.Bidirectional(tfkl.LSTM(256, return_sequences=True), name = \"LSTM2\"))\n    model.add(SeqSelfAttention(\n        #attention_width=24,\n        #attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n        #attention_activation='softmax',\n        name = \"LSTM2-a\"\n    ))\n    model.add(tfkl.Dropout(.3, seed=seed, name = \"LSTM2-d\"))\n    model.add(tfkl.Bidirectional(tfkl.LSTM(256, return_sequences=True), name = \"LSTM3\"))\n    model.add(SeqSelfAttention(\n        #attention_width=36,\n        #attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n        #attention_activation='softmax',\n        name = \"LSTM3-a\"\n    ))\n    model.add(tfkl.Dropout(.3, seed=seed, name = \"LSTM3-d\"))\n    \n    model.add(tfkl.GlobalAveragePooling1D())\n    \n    model.add(tfkl.Dropout(.5, seed=seed))\n\n    # Classifier\n    model.add(tfkl.Dense(\n        128, activation='relu',\n        kernel_regularizer=tfk.regularizers.L1(0.01),\n        activity_regularizer=tfk.regularizers.L2(0.01)\n    ))\n    model.add(tfkl.Dense(classes, activation='softmax'))\n\n    # Connect input and output through the Model class\n    #model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    # Compile the model\n    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n\n    # Return the model\n    return model","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = build_LSTM_classifier(input_shape, classes)\n# model = build_BiLSTM_classifier(input_shape, classes)\n# model = build_1DCNN_classifier(input_shape, classes)   # BEST ONE\n# model = build_old1DCNN_classifier(input_shape, classes)\n#model = build_attention1_classifier(input_shape, classes)\nmodel = build_attention2_classifier(input_shape, classes)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"### Adapt preprocessing layers","metadata":{}},{"cell_type":"code","source":"model.get_layer(\"Norm\").adapt(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train","metadata":{}},{"cell_type":"code","source":"#model.get_layer(\"LSTM1\").trainable = False\n#model.get_layer(\"LSTM1-a\").trainable = False\n#model.get_layer(\"LSTM1-d\").trainable = False\n\n#model.get_layer(\"LSTM2\").trainable = False\n#model.get_layer(\"LSTM2-a\").trainable = False\n#model.get_layer(\"LSTM2-d\").trainable = False\n\n#model.get_layer(\"LSTM3\").trainable = False\n#model.get_layer(\"LSTM3-a\").trainable = False\n#model.get_layer(\"LSTM3-d\").trainable = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    x = x_train,\n    y = y_train,\n    validation_data = (x_val, y_val),\n    batch_size = batch_size,\n    epochs = epochs,\n    #class_weight = class_weights,\n    callbacks = [\n        #tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=30, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=20, factor=0.5, min_lr=1e-4),\n        tf.keras.callbacks.ModelCheckpoint(\n            filepath=\"/kaggle/working/cp1/cp\",\n            save_weights_only=True,\n            monitor='val_accuracy',\n            mode='max',\n            save_best_only=True\n        )\n    ]\n).history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Results","metadata":{}},{"cell_type":"code","source":"model.load_weights(\"/kaggle/working/cp1\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_classes_performance(model, x_val, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save","metadata":{}},{"cell_type":"code","source":"#import shutil\n#model.load_weights(\"/kaggle/working/cp\")\n#model.save('ANN_Homework2_Model')\n#shutil.make_archive(\"ANN_Homework2_Model\", 'zip', '/kaggle/working/ANN_Homework2_Model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}